---
layout: post
title:  "Parquet"
date:   2021-05-24
excerpt: "Parquet"

comments: false
---
# parqeut
* 컬럼 기반으로 저장하는 파일 포맷
* 구글의 드러멜(dremel)에 기반 



# parquet 구조

* 헤더 : 4바이트 매직 숫자인 PAR1만 포함
* 블록 (하나 이상)
    * 행 그룹 저장
        * 행 그룹 : 행에 대한 컬럼 데이터 포함한 컬럼 청크
        * 컬럼 청크의 데이터 - 페이지에 기록
    * block > cloumn chuck > page
    * 블록 크기는 HDFS 블록크기보다 크면 안됨 (HDFS 블록 기본 : 128MB)
        * 페이지 : 동일한 컬럼의 값만 포함하며, parquet 파일의 최소 저장 단위
            * 인코딩 및 압축 시 페이지 단위로 수행
            * 페이지 크기 ↓  - 페이지 수 증가  - 메타데이터 증가 저장용량 및 처리 시간 증가
  꼬리말
    : 포맷버전, 스키마, 추가 키-값 쌍, 파일의 모든 블록에 대한 메타데이터와 같은 정보를 포함


# parquet 설정
파일의 속성은 파일 기록 시점에 정해짐

ParquetOutputFormat 속성
* parquet.block.size : 블록의 바이트 크기(행 그룹, int, default: 128MB)
* parquet.page.size : 페이지의 바이트 크기 (int, default: 1MB)
* parquet.dictionary.page.size : 일반 인코딩으로 돌아가기 전의 사전의 최대 허용 바이트 크기 (int, default: 1MB)
* parquet.enable.dictionary : 사전 인코딩 사용여부 (bool, default: True)
* parquet.compression : 사용할 압축 종류 (string)




# 빅데이터 처리에 컬럼기반 포멧이 우세한 이유
빅데이터 처리는 분산 처리를 기본으로 하며 각 노드에 저장된 파일을 빨리 읽는 게 중요함. 컬럼 기반의 포멧은 이러한 조건이 행 기반 포멧보다 우수하여 빅데이터 처리도 우세함

## 컬럼 기반 포멧의 장점
1. 압축률 우수 - 파일 크기 작음<br>
    직렬화된 컬럼은 하나의 타입을 가짐. 따라서 행 기반에서는 이용할 수 없던 압축 방식을 적용해서 압축률이 우수하여 파일 크기가 작음
2. 인코딩 효율이 좋음 - 1번의 장점 덕분에
3. 쿼리 성능이 좋음 <br>
why ? 필요한 컬럼만 읽을 수 있어서 I/O 사용률이 낮으며 행 기반 포맷에 비해서 성능이 더 좋음

# orc vs parquet vs Avro

|ORC |parqet|Avro|
|:--------:|:--------:|:--------:|
|row Columnar|row Columnar|row-based|
|HIVE,Presto(범용스토리지 포멧X)|Spark,Impala,Arrow| kafka, druid |



## avro (row 기반)
Apache의 Hadoop 프로젝트 내에서 개발 된 원격 프로 시저 호출 및 데이터 직렬화 프레임 워크
* 데이터 유형 및 프로토콜을 정의하는데 Json을 사용하고, 압축된 이진형식으로 데이터를 직렬화함
* 장점
1. 모든 필드에 접근해야할 때 유용
2. 블록 압축을 지원하며 분할 가능
3. 스트리밍 데이터에서 읽을 수 있음



### 참고
https://github.com/apache/parquet-format
